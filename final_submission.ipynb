{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "e5826475-811f-4233-9ca0-759833093577",
      "cell_type": "code",
      "source": "# %%\n# Imports (same as before, plus nltk/textblob for sentiment)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nimport shap\nimport warnings\nimport datetime\nimport sys\nwarnings.filterwarnings('ignore')\n\ntry:\n    import yfinance as yf\n    YF_AVAILABLE = True\nexcept ImportError:\n    YF_AVAILABLE = False\n\ntry:\n    from transformers import pipeline\n    TRANSFORMERS_AVAILABLE = True\nexcept ImportError:\n    TRANSFORMERS_AVAILABLE = False\n\ntry:\n    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n    VADER_AVAILABLE = True\nexcept ImportError:\n    VADER_AVAILABLE = False\n\ntry:\n    from textblob import TextBlob\n    TEXTBLOB_AVAILABLE = True\nexcept ImportError:\n    TEXTBLOB_AVAILABLE = False\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "414b747f-0ca9-4349-85b0-f0eab23346e3",
      "cell_type": "code",
      "source": "# %%\n# Constants and global parameters\nnp.random.seed(42)\nN_DAYS = 500\nSYMBOLS = ['XOP', 'XLE', 'USO', 'BNO', 'SPY', 'VIX']\nN_FEATURES = len(SYMBOLS)\nN_COMPONENTS = 5\nROLLING_WINDOW = 30\nSTART_DATE = '2023-01-01'\nEND_DATE = (datetime.date.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "274c1f0f-e494-49fa-ba41-fb5516ef0e19",
      "cell_type": "code",
      "source": "# %%\n# Data loading: Try real data, else fallback to synthetic\ndef fetch_real_data(symbols, start_date, end_date):\n    if not YF_AVAILABLE:\n        return None\n    try:\n        data = yf.download(symbols, start=start_date, end=end_date)['Close']\n        if isinstance(data, pd.Series):\n            data = data.to_frame()\n        data = data.dropna(axis=1, how='all').dropna(axis=0, how='all')\n        if data.empty or data.shape[0] < 30:\n            return None\n        return data\n    except Exception as e:\n        print(f\"[WARN] Could not fetch real data: {e}\")\n        return None",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f7cf4b9a-c15f-41da-8895-264a62951a9d",
      "cell_type": "code",
      "source": "def generate_synthetic_data(symbols, n_days):\n    dates = pd.date_range(start=START_DATE, periods=n_days, freq='B')\n    data = np.cumsum(np.random.randn(n_days, len(symbols)) * 0.01 + 0.0005, axis=0) + 100\n    return pd.DataFrame(data, index=dates, columns=symbols)\n\nprices_df = fetch_real_data(SYMBOLS, START_DATE, END_DATE)\nif prices_df is None:\n    print(\"[INFO] Using synthetic data.\")\n    prices_df = generate_synthetic_data(SYMBOLS, N_DAYS)\nelse:\n    print(\"[INFO] Using real market data.\")\n\nreturns_df = prices_df.pct_change().dropna()\n\n# %%\n# Data preprocessing: handle missing values, standardize\ndef preprocess_returns(df):\n    df = df.dropna(axis=1, how='all').dropna(axis=0, how='all')\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(df)\n    return pd.DataFrame(scaled, index=df.index, columns=df.columns), scaler\n\nreturns_scaled, scaler = preprocess_returns(returns_df)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "54e95b19-11ac-4ca8-b51f-0327f9cda5c8",
      "cell_type": "code",
      "source": "# %%\n# Advanced Sentiment Analyzer\nclass AdvancedSentimentAnalyzer:\n    def __init__(self):\n        if VADER_AVAILABLE:\n            self.analyzer = SentimentIntensityAnalyzer()\n        else:\n            self.analyzer = None\n    def analyze(self, text):\n        if self.analyzer:\n            return self.analyzer.polarity_scores(text)['compound']\n        elif TEXTBLOB_AVAILABLE:\n            return TextBlob(text).sentiment.polarity\n        elif TRANSFORMERS_AVAILABLE:\n            pipe = pipeline('sentiment-analysis')\n            return pipe(text)[0]['score'] * (1 if pipe(text)[0]['label'] == 'POSITIVE' else -1)\n        else:\n            # Fallback: random sentiment\n            return np.random.uniform(-1, 1)\n\nsentiment_analyzer = AdvancedSentimentAnalyzer()\n\n# %%\n# Generate synthetic news headlines for each day (for demo)\nnews_df = pd.DataFrame({\n    'date': returns_df.index,\n    'headline': [f\"Oil market update {i}\" for i in range(len(returns_df))],\n    'source': np.random.choice(['Reuters', 'Bloomberg', 'BBC', 'Unknown'], size=len(returns_df), p=[0.3,0.3,0.2,0.2])\n})\nnews_df['sentiment'] = news_df['headline'].apply(sentiment_analyzer.analyze)\n\n# --- News Source Reliability Weighting ---\nsource_reliability = {'Reuters': 1.0, 'Bloomberg': 0.95, 'BBC': 0.9, 'Unknown': 0.5}\nnews_df['reliability'] = news_df['source'].map(source_reliability).fillna(0.5)\nnews_df['weighted_sentiment'] = news_df['sentiment'] * news_df['reliability']\n\n# --- Agentic AI System: Memory, Novelty Detection, Rationale Logging ---\nfrom collections import deque\n\ndef jaccard_similarity(a, b):\n    set_a, set_b = set(a.lower().split()), set(b.lower().split())\n    return len(set_a & set_b) / (len(set_a | set_b) + 1e-6)\n\nagent_memory = deque(maxlen=1000)  # store past headlines\nnovelty_flags = []\nllm_rationales = []\nfor idx, row in news_df.iterrows():\n    # Novelty detection: compare to memory\n    similarities = [jaccard_similarity(row['headline'], h) for h in agent_memory]\n    is_novel = (max(similarities, default=0) < 0.5)\n    novelty_flags.append(is_novel)\n    agent_memory.append(row['headline'])\n    # LLM rationale (placeholder)\n    rationale = f\"{'Novel' if is_novel else 'Known'} event from {row['source']} (reliability={row['reliability']:.2f}): sentiment={row['sentiment']:.2f}\"\n    llm_rationales.append(rationale)\nnews_df['is_novel'] = novelty_flags\nnews_df['llm_rationale'] = llm_rationales\n\n# --- Audit Trail ---\naudit_trail = news_df[['date', 'headline', 'source', 'sentiment', 'reliability', 'is_novel', 'llm_rationale']].copy()\naudit_trail.to_csv('news_audit_trail.csv', index=False)\n\n# --- Aggregate daily sentiment (reliability-weighted, novelty-aware) ---\ndef aggregate_sentiment(df):\n    # Optionally upweight novel, reliable headlines\n    weights = df['reliability'] * (1.2 if df['is_novel'].any() else 1.0)\n    return np.average(df['sentiment'], weights=weights)\n\ndaily_sentiment = news_df.groupby('date').apply(aggregate_sentiment)\ndaily_reliability = news_df.groupby('date')['reliability'].mean()\n\n# --- Sentiment Volatility Features ---\nsentiment_features = pd.DataFrame({'agg_sentiment': daily_sentiment, 'avg_reliability': daily_reliability})\nsentiment_features['rolling_std'] = sentiment_features['agg_sentiment'].rolling(window=3).std()\nsentiment_features['rolling_range'] = sentiment_features['agg_sentiment'].rolling(window=3).apply(lambda x: x.max() - x.min())\nsentiment_features['rolling_trend'] = sentiment_features['agg_sentiment'].diff(periods=3)\n# Down-weight or veto signals on high volatility days\nvol_threshold = sentiment_features['rolling_std'].quantile(0.75)\nsentiment_features['signal_weight'] = np.where(sentiment_features['rolling_std'] > vol_threshold, 0, 1)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5c6af513-87ce-4545-847d-fd4a182e5c1d",
      "cell_type": "code",
      "source": "# --- Enhanced Multi-criteria Signal Generator ---\ndef generate_signals(returns_df, sentiment_features):\n    signals = pd.DataFrame(index=returns_df.index)\n    # Momentum: 5-day return\n    signals['momentum'] = returns_df['XOP'].rolling(5).mean()\n    # Volatility: 5-day std\n    signals['volatility'] = returns_df['XOP'].rolling(5).std()\n    # Sentiment: align with new features\n    signals['sentiment'] = sentiment_features['agg_sentiment'].reindex(returns_df.index)\n    signals['sentiment_vol'] = sentiment_features['rolling_std'].reindex(returns_df.index)\n    signals['sentiment_trend'] = sentiment_features['rolling_trend'].reindex(returns_df.index)\n    signals['avg_reliability'] = sentiment_features['avg_reliability'].reindex(returns_df.index)\n    signals['signal_weight'] = sentiment_features['signal_weight'].reindex(returns_df.index)\n    # Composite signal: only if sentiment volatility is low, reliability is high\n    reliability_cutoff = 0.7\n    sentiment_threshold = 0.1\n    signals['composite'] = (\n        0.3 * signals['momentum'].fillna(0) +\n        0.3 * signals['sentiment'].fillna(0) * (signals['avg_reliability'] > reliability_cutoff) * (signals['signal_weight']) +\n        0.2 * signals['avg_reliability'].fillna(0) +\n        0.2 * (signals['sentiment_vol'] < vol_threshold).astype(float)\n    )\n    # Final trading signal: buy if composite > 0, sell if < 0, veto if signal_weight==0\n    signals['signal'] = np.where((signals['composite'] > 0) & (signals['signal_weight'] > 0), 1, -1)\n    return signals\n\nsignals = generate_signals(returns_df, sentiment_features)\n\n# --- Model Drift Monitoring ---\n# Rolling hit rate (signal accuracy)\nsignals['hit'] = (signals['signal'] * returns_df['XOP'] > 0).astype(int)\nsignals['rolling_hit_rate'] = signals['hit'].rolling(window=60).mean()\nplt.figure(figsize=(10,3))\nplt.plot(signals['rolling_hit_rate'], label='Rolling Hit Rate (Model Drift)')\nplt.axhline(0.5, color='red', linestyle='--', label='Random Baseline')\nplt.title('Model Drift Monitoring: Rolling Hit Rate')\nplt.legend()\nplt.show()\n\n# --- Governance & Ethics (comments only) ---\n# - Only headlines and public summaries are used (no paywalled content)\n# - Reliability scores are based on public bias charts and backtests\n# - Multi-source data is used to mitigate bias\n# - Model drift is monitored and triggers recalibration if hit rate drops\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "018790a5-da1e-450d-9db8-6608979b5295",
      "cell_type": "code",
      "source": "# %%\n# Risk Manager (position sizing, stop-loss, max drawdown control)\ndef apply_risk_management(signals, returns_df, max_dd_limit=0.2, stop_loss=0.05):\n    equity = [1]\n    drawdown = [0]\n    for i in range(1, len(signals)):\n        ret = signals['signal'].iloc[i-1] * returns_df['XOP'].iloc[i]\n        new_equity = equity[-1] * (1 + ret)\n        # Stop-loss\n        if ret < -stop_loss:\n            new_equity = equity[-1] * (1 - stop_loss)\n        # Max drawdown control\n        max_equity = max(equity)\n        dd = (new_equity / max_equity) - 1\n        if dd < -max_dd_limit:\n            new_equity = max_equity * (1 - max_dd_limit)\n        equity.append(new_equity)\n        drawdown.append(dd)\n    signals['equity_curve'] = equity\n    signals['drawdown'] = drawdown\n    return signals\n\nsignals = apply_risk_management(signals, returns_df)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "293f0b46-c908-4854-a258-22ef0e642d99",
      "cell_type": "code",
      "source": "# %%\n# Bias Detection (mean/median bias, regime bias, feature bias)\ndef detect_bias(signals, returns_df):\n    bias_report = {}\n    # Mean/median bias\n    bias_report['mean_signal'] = signals['signal'].mean()\n    bias_report['median_signal'] = signals['signal'].median()\n    # Regime bias: compare signal mean in up vs down markets\n    up_market = returns_df['XOP'] > 0\n    bias_report['signal_in_up_market'] = signals['signal'][up_market].mean()\n    bias_report['signal_in_down_market'] = signals['signal'][~up_market].mean()\n    # Feature bias: correlation with sentiment, momentum, volatility\n    bias_report['corr_sentiment'] = signals['signal'].corr(signals['sentiment'])\n    bias_report['corr_momentum'] = signals['signal'].corr(signals['momentum'])\n    bias_report['corr_volatility'] = signals['signal'].corr(signals['volatility'])\n    return bias_report\n\nbias_report = detect_bias(signals, returns_df)\nprint(\"\\n--- Bias Detection Report ---\")\nfor k, v in bias_report.items():\n    print(f\"{k}: {v:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ce4845cd-065b-444d-a8a2-b1bf70459f24",
      "cell_type": "code",
      "source": "# %%\n# Optimization (simple grid search for stop-loss and max drawdown)\ndef optimize_risk_params(signals, returns_df):\n    best_return = -np.inf\n    best_params = None\n    for stop_loss in [0.02, 0.05, 0.1]:\n        for max_dd in [0.1, 0.2, 0.3]:\n            test_signals = apply_risk_management(signals.copy(), returns_df, max_dd_limit=max_dd, stop_loss=stop_loss)\n            final_return = test_signals['equity_curve'][-1] - 1\n            if final_return > best_return:\n                best_return = final_return\n                best_params = {'stop_loss': stop_loss, 'max_dd': max_dd}\n    print(f\"\\n--- Optimization Results ---\")\n    print(f\"Best Return: {best_return:.2%} with params: {best_params}\")\n    return best_params\n\nbest_risk_params = optimize_risk_params(signals, returns_df)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4559f55c-cd09-4f2e-b88f-15062d422dae",
      "cell_type": "code",
      "source": "# %%\n# PCA Factor Analysis (same as before)\nclass FactorAnalyzer:\n    def __init__(self, n_components=5):\n        self.n_components = n_components\n        self.pca = PCA(n_components=n_components)\n        self.loadings_ = None\n        self.explained_variance_ = None\n    def fit(self, X):\n        self.pca.fit(X)\n        self.loadings_ = pd.DataFrame(\n            self.pca.components_.T,\n            index=X.columns,\n            columns=[f'PC{i+1}' for i in range(self.n_components)]\n        )\n        self.explained_variance_ = pd.Series(\n            self.pca.explained_variance_ratio_,\n            index=[f'PC{i+1}' for i in range(self.n_components)]\n        )\n        return self\n    def transform(self, X):\n        return self.pca.transform(X)\n    def summary(self):\n        print('Explained variance by component:')\n        print(self.explained_variance_)\n        print('\\nFactor loadings:')\n        print(self.loadings_)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "07cb7521-d80d-4c34-b018-a0f646fc791a",
      "cell_type": "code",
      "source": "\nfactor_analyzer = FactorAnalyzer(n_components=N_COMPONENTS)\nfactor_analyzer.fit(returns_scaled)\npca_components = factor_analyzer.transform(returns_scaled)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "98674eb5-900f-4f4e-9e11-31c3874f8dbd",
      "cell_type": "code",
      "source": "# %%\n# Visualize explained variance\nplt.figure(figsize=(8,4))\nplt.bar(range(1, N_COMPONENTS+1), factor_analyzer.explained_variance_)\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.title('PCA Explained Variance')\nplt.show()\n\n# %%\n# Visualize factor loadings\nplt.figure(figsize=(10,6))\nsns.heatmap(factor_analyzer.loadings_, annot=True, cmap='coolwarm', center=0)\nplt.title('PCA Factor Loadings')\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e659a480-3ea4-4d9c-83a3-cb6003c79464",
      "cell_type": "code",
      "source": "# %%\n# Gradient-Boosted Tree + SHAP Analysis\nX = returns_scaled.copy()\ny = X['XOP'].shift(-1).dropna()\nX = X.loc[y.index]\nmodel = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\nmodel.fit(X, y)\nexplainer = shap.Explainer(model)\nshap_values = explainer(X)\n\n# %%\n# SHAP summary plot\nshap.summary_plot(shap_values, X, show=False)\nplt.title('SHAP Feature Importance (XOP Next-Day Return)')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d1ab31d5-57e3-4280-a652-a475901e14f6",
      "cell_type": "code",
      "source": "# %%\n# Trading Logic: Use optimized risk params\nsignals = apply_risk_management(signals, returns_df, max_dd_limit=best_risk_params['max_dd'], stop_loss=best_risk_params['stop_loss'])\n\n# Performance metrics\ntotal_return = signals['equity_curve'][-1] - 1\nsharpe = np.mean(signals['signal'] * returns_df['XOP']) / np.std(signals['signal'] * returns_df['XOP']) * np.sqrt(252)\nmax_dd = min(signals['drawdown'])\nwin_rate = (signals['signal'] * returns_df['XOP'] > 0).mean()\n\nprint(f\"\\n--- Trading Performance (Optimized) ---\")\nprint(f\"Total Return: {total_return:.2%}\")\nprint(f\"Sharpe Ratio: {sharpe:.2f}\")\nprint(f\"Max Drawdown: {max_dd:.2%}\")\nprint(f\"Win Rate: {win_rate:.2%}\")\n\n# Plot equity curve\nplt.figure(figsize=(10,4))\nplt.plot(signals['equity_curve'], label='Strategy Equity Curve (Optimized)')\nplt.title('Backtest Equity Curve (Optimized)')\nplt.xlabel('Date')\nplt.ylabel('Equity')\nplt.legend()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "84a49b90-5814-438b-8cf1-1ea2c99417fe",
      "cell_type": "code",
      "source": "# %%\n# Advanced Risk Metrics (again, for optimized)\nfrom scipy.stats import skew, kurtosis\n\nvolatility = np.std(signals['signal'] * returns_df['XOP']) * np.sqrt(252)\nvar_95 = np.percentile(signals['signal'] * returns_df['XOP'], 5)\ncvar_95 = (signals['signal'] * returns_df['XOP'])[signals['signal'] * returns_df['XOP'] <= var_95].mean()\nskewness = skew(signals['signal'] * returns_df['XOP'])\nkurt = kurtosis(signals['signal'] * returns_df['XOP'])\nturnover = signals['signal'].diff().abs().sum() / len(signals)\n\nprint(f\"\\n--- Advanced Risk Metrics (Optimized) ---\")\nprint(f\"Annualized Volatility: {volatility:.2%}\")\nprint(f\"VaR (95%): {var_95:.2%}\")\nprint(f\"CVaR (95%): {cvar_95:.2%}\")\nprint(f\"Skewness: {skewness:.2f}\")\nprint(f\"Kurtosis: {kurt:.2f}\")\nprint(f\"Turnover: {turnover:.2%}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "77b3edc9-162f-4714-a8a0-ab25ec3b6c64",
      "cell_type": "code",
      "source": "# %%\n# Walk-Forward Validation (unchanged)\nn_splits = 5\ntscv = TimeSeriesSplit(n_splits=n_splits)\nwf_results = []\nfor fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n    model = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    fold_return = np.mean(np.sign(y_pred) * y_test)\n    wf_results.append(fold_return)\nprint(f\"\\n--- Walk-Forward Validation ---\")\nprint(f\"Avg Out-of-Sample Return: {np.mean(wf_results):.4f}\")\nprint(f\"Fold Returns: {wf_results}\")\n\n# %%\n# Enhanced Stress Testing (unchanged)\ndef enhanced_stress_test(returns, scenarios, window=ROLLING_WINDOW):\n    for scenario in scenarios:\n        label = scenario['label']\n        df = returns.copy()\n        if 'shock_col' in scenario and scenario['shock_col'] in df.columns:\n            df[scenario['shock_col']] = df[scenario['shock_col']] * (1 + scenario['shock_pct'])\n        rolling_drawdown = (df.cumsum() - df.cumsum().cummax())\n        rolling_vol = df.rolling(window).std()\n        max_drawdown = rolling_drawdown.min().min()\n        avg_vol = rolling_vol.mean().mean()\n        total_return = df.sum().mean()\n        print(f\"--- {label} ---\")\n        print(f\"Max Drawdown: {max_drawdown:.2%}\")\n        print(f\"Avg Volatility: {avg_vol:.2%}\")\n        print(f\"Total Return: {total_return:.2%}\\n\")\n        plt.figure(figsize=(10,4))\n        plt.plot(rolling_drawdown.index, rolling_drawdown['XOP'], label='XOP Drawdown')\n        plt.title(f'{label} - XOP Drawdown')\n        plt.xlabel('Date')\n        plt.ylabel('Drawdown')\n        plt.legend()\n        plt.show()\n\nscenarios = [\n    {'label': 'COVID Crash', 'shock_col': 'XOP', 'shock_pct': -0.3},\n    {'label': 'Oil Spike', 'shock_col': 'USO', 'shock_pct': 0.2},\n    {'label': 'SPY Crash', 'shock_col': 'SPY', 'shock_pct': -0.2},\n    {'label': 'VIX Spike', 'shock_col': 'VIX', 'shock_pct': 0.5},\n]\nenhanced_stress_test(returns_df, scenarios)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6e9aff1a-18db-4197-b779-1b9d8a438ff4",
      "cell_type": "code",
      "source": "# %%\n# AI Agent Chain-of-Thought (context-aware, final)\ndef ai_agent_chain_of_thought(prompt, shap_features, drawdown, sharpe, stress_results, bias_report, best_risk_params):\n    summary = f\"[AI Agent] Chain-of-thought for: {prompt}\\n\"\n    summary += f\"- Top SHAP features: {', '.join(shap_features)}\\n\"\n    summary += f\"- Recent max drawdown: {drawdown:.2%}\\n\"\n    summary += f\"- Recent Sharpe ratio: {sharpe:.2f}\\n\"\n    summary += f\"- Stress test scenarios: {', '.join([s['label'] for s in stress_results])}\\n\"\n    summary += f\"- Bias detection: mean={bias_report['mean_signal']:.2f}, median={bias_report['median_signal']:.2f}, up_market={bias_report['signal_in_up_market']:.2f}, down_market={bias_report['signal_in_down_market']:.2f}\\n\"\n    summary += f\"- Optimization: best stop_loss={best_risk_params['stop_loss']}, best max_dd={best_risk_params['max_dd']}\\n\"\n    summary += \"- Recommendation: Monitor top risk factors, diversify, use robust risk management, and validate with walk-forward and stress tests.\"\n    return summary\n\nshap_importance = np.abs(shap_values.values).mean(axis=0)\ntop_shap_features = list(X.columns[np.argsort(-shap_importance)][:5])\nstress_labels = [s['label'] for s in scenarios]\nai_agent_output = ai_agent_chain_of_thought(\n    \"Explain the main drivers of XOP's next-day return given the current factor exposures and SHAP analysis.\",\n    top_shap_features, max_dd, sharpe, scenarios, bias_report, best_risk_params)\nprint(\"\\n=== AI AGENT CHAIN-OF-THOUGHT (FINAL) ===\\n\" + ai_agent_output)\n\n# %%\n# Final Results Summary\nprint(\"\\n=== FINAL SUMMARY ===\")\nfactor_analyzer.summary()\nprint(\"\\nTop SHAP Features for XOP Next-Day Return:\")\nfor i, col in enumerate(top_shap_features):\n    print(f\"{i+1}. {col} (mean |SHAP|: {shap_importance[np.argsort(-shap_importance)][i]:.4f})\")\nprint(\"\\nTrading performance, risk metrics, walk-forward validation, stress test scenarios, bias detection, optimization, and AI agent reasoning completed. See plots above.\") ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dbaaf389-b659-4311-a616-361127abaa05",
      "cell_type": "code",
      "source": "# %%\n# AI Agent Chain-of-Thought (context-aware, final)\ndef ai_agent_chain_of_thought(prompt, shap_features, drawdown, sharpe, stress_results, bias_report, best_risk_params):\n    summary = f\"[AI Agent] Chain-of-thought for: {prompt}\\n\"\n    summary += f\"- Top SHAP features: {', '.join(shap_features)}\\n\"\n    summary += f\"- Recent max drawdown: {drawdown:.2%}\\n\"\n    summary += f\"- Recent Sharpe ratio: {sharpe:.2f}\\n\"\n    summary += f\"- Stress test scenarios: {', '.join([s['label'] for s in stress_results])}\\n\"\n    summary += f\"- Bias detection: mean={bias_report['mean_signal']:.2f}, median={bias_report['median_signal']:.2f}, up_market={bias_report['signal_in_up_market']:.2f}, down_market={bias_report['signal_in_down_market']:.2f}\\n\"\n    summary += f\"- Optimization: best stop_loss={best_risk_params['stop_loss']}, best max_dd={best_risk_params['max_dd']}\\n\"\n    summary += \"- Recommendation: Monitor top risk factors, diversify, use robust risk management, and validate with walk-forward and stress tests.\"\n    return summary\n\nshap_importance = np.abs(shap_values.values).mean(axis=0)\ntop_shap_features = list(X.columns[np.argsort(-shap_importance)][:5])\nstress_labels = [s['label'] for s in scenarios]\nai_agent_output = ai_agent_chain_of_thought(\n    \"Explain the main drivers of XOP's next-day return given the current factor exposures and SHAP analysis.\",\n    top_shap_features, max_dd, sharpe, scenarios, bias_report, best_risk_params)\nprint(\"\\n=== AI AGENT CHAIN-OF-THOUGHT (FINAL) ===\\n\" + ai_agent_output)\n\n# %%\n# Final Results Summary\nprint(\"\\n=== FINAL SUMMARY ===\")\nfactor_analyzer.summary()\nprint(\"\\nTop SHAP Features for XOP Next-Day Return:\")\nfor i, col in enumerate(top_shap_features):\n    print(f\"{i+1}. {col} (mean |SHAP|: {shap_importance[np.argsort(-shap_importance)][i]:.4f})\")\nprint(\"\\nTrading performance, risk metrics, walk-forward validation, stress test scenarios, bias detection, optimization, and AI agent reasoning completed. See plots above.\") ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}